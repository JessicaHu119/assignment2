---
title: "assignment_2"
author: "u7457284"
date: "2022-10-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Correct analysis of Clark et al. (2020) data (i.e., OA_activitydat_20190302_BIOL3207.csv) to generate the summary statistics (means, SD, N) for each of the fish species’ average activity for each treatment.

### By adding the link in the `()` brackets it will hyperlink "My GitHub Repository". This is how to hyperlink in Rmarkdown documents.

[My GitHub Repository](https://github.com/JessicaHu119/Shiqi-Hu_assigenment2.git)

### Read all files

```{r}
library(tidyverse)
OA <- read_csv("./Data/OA_activitydat_20190302_BIOL3207.csv")
clark_paper <- read_csv("./Data/clark_paper_data.csv")
ocean_meta <- read_csv("./Data/ocean_meta_data.csv")
```

### Calculate means, SD, N for each of the fish species’ average activity for each treatment 

```{r}
data_OA <- OA %>% group_by(species, treatment) %>%
              summarise(mean = mean(activity, na.rm = TRUE),
                        sd = sd(activity, na.rm = TRUE),
                        n = length(unique(animal_id))) %>%
              rename(Species = "species")
```

# 2. Through coding, merge the summary statistics generated from 1) with the metadata (i.e., clark_paper_data.csv) from Clark et al. (2020).

```{r}
# Merge the table and add new columns and reduce rows
cb <- cbind(clark_paper, data_OA)
new <- pivot_wider(cb, names_from = treatment,
                     names_glue = "{treatment}_{.value}",
                     values_from = c("mean", "sd", "n"))
```

# 3. Through coding, correctly merge the combined summary statistics and metadata from Clark et al. (2020) (output from 1 & 2) into the larger meta-analysis dataset (i.e., ocean_meta_data.csv).

```{r}
# Displays the rows and columns of the table
dim(ocean_meta)
dim(new)
```

### Do some renaming of colnames so they match ocean_meta

```{r}
# Rename the column
new1 <-  new %>% rename("oa.mean" = CO2_mean,
                            "oa.sd" = CO2_sd,
                            "oa.n" = CO2_n,
                            "ctrl.mean" = control_mean,
                            "ctrl.sd" = control_sd,
                            "ctrl.n" = control_n)
```

### Reorder col names based on names in ocean_meta

```{r}
new1 <- new1[names(ocean_meta)]
```

### Check columns are in same order

```{r}
colnames(ocean_meta) == colnames(new1)
```

### Bind the two dataframes

```{r}
total <- rbind(ocean_meta, new1)
total <- total %>%  mutate(residual = 1:n())
total <- total[complete.cases(total),]
```

# 4. Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s escalc() function.

### Load the necessary R Packages
```{r}
library(pacman)
library(orchaRd)
```

### For'measure="ROM" , the log is taken of the ratio of means, which makes this outcome measure symmetric around 0 and yields a coresponding sampling distribution that is closer to normality. Hence, this measure cannot be computed when 'm1i' and 'm2i' have opposite signs.

```{r}
lnRR <- metafor::escalc(measure = "ROM", m1i = ctrl.mean, m2i = oa.mean, sd1i = ctrl.sd, sd2i = oa.sd, n1i = ctrl.n, n2i = oa.n, data = total,
    var.names = c("lnRR", "V_lnRR")) 
```

# 5. Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor’s rma.mv() function.

```{r}
#Integrated model
MLMA <- metafor::rma.mv( lnRR~1, V = V_lnRR, random=list(~1|Study, ~1|residual), method = "REML", test = "t", dfs = "contain", data=lnRR)
MLMA
```

# 6. Written paragraph of the findings and what they mean which is supported with a figure. The paragraph should include: 

## Overall meta-analytic mean
We want to know what the overall meta-analytic mean effect size across the studies actually is estimated to be. We can see that from the model by extracting the intercept (labeled ‘estimate’ in the model output). Recall that the model is just an object that stores all the values for us.

We can extract the estimate, it is estimated to be -0.186, which tells us that the mean lnRR value is negative, but there is a rather weak overall association between physiology and dispersal / movement when we pool across all studies.

If we want to convert the overall meta-analytic mean back to the correlation coefficient we can use the predict function to help with that. We can use the transf argument within the function:

```{r}
predict(MLMA, transf = "transf.ztor")
```

There are LOTS of different transformations that can be done depending on which effect size you plan on using. Get familiar with the predict function and the transf argument by looking at it’s help file.

## Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals).

The 95% confidence intervals for the overall meta-analysis mean are from -0.350 to  0.098. At the same time, 95% of the time, we would expect the true mean to fall between -0.311 and 0.189 lnRR values.

We can also see that there is a significantly smaller estimate than a correlation of 0, which we can see from the p-value being > 0.05. Actually, p-value is 0.2673. It means that the experimental group of each experiment does not differ much from the control group.

### Measures of heterogeneity in effect size estimates across studies (i.e., I2 and/or prediction intervals - see predict() function in metafor)

```{r}
## Calculate I2
i2_vals <- orchaRd::i2_ml(MLMA)
## Make a pretty table. First, lets clean up the names of the different I2. Estimates. Lets remove I2_. It's a string, so, we can use some regular. Expressions to fix that. `gsub` is pretty useful. You put a pattern in and. Tell it what you would like to replace the text with. In this case, just. Blank will do! Then, we'll make the first letter of what is left, capitalised.
i2 <- tibble(type = firstup(gsub("I2_", "", names(i2_vals))), I2 = i2_vals)
i2
# Now, lets make a pretty table.
library(flextable)
ft <- flextable(i2)%>%
    align(part = "header", align = "center") %>%
    compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>%
    compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")),
        as_b("(%)"))) 
ft
```

Overall, we have highly heterogeneous effect size data because sampling variation only contributes to 0 of the total variation in effects.

From the multilevel meta-analytic model we find that only 5.585% of the total variation in effect size estimates is the result of differences between studies.

## Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure

There are some nice graphical approaches that capture the meta-analytic model results. We can use “orchard” plots which are modifications to what are called forest plots. We can do this with the orchaRd R package:

```{r}
# Make an orchard plot using the model object
orchaRd::orchard_plot(MLMA, mod = 1, group = "Study", data = lnRR,
    xlab = "log response ratio (lnRR)", angle = 45)
```

Figure 1: Orchard plot showing the mean lnRR for correlation coefficients estimated between physiology and activity, dispersal and behaviour. k = the number of effect sizes and the number of studies are in brackets. The size of the effect is scaled by the precision of each effect size value, which is 1 / the sqrt(v_lnRR)

# 7. Funnel plot for visually assessing the possibility of publication bias.

```{r}
# Lets make a funnel plot to visualize the data in relation to the precision,
metafor::funnel(x = lnRR$lnRR, vi = lnRR$V_lnRR, yaxis = "seinv", xlim = c(-2,2), ylim = c(0.1,100),
    digits = 2, level = c(0.1, 0.05, 0.01), shade = c("white", "gray25", "gray 55", "gray95"),
    las = 1, xlab = "Correlation Coefficient (r)", atransf = tanh, legend = TRUE, col="black")
```

Figure 2: Funnel plot depicting the correlation between lnRR and V_lnRR as a function of precision (1 / SE). The dotted lines are the theoretical 95% sampling variance intervals - the interval with which we expect effect size estimates to fall within if only sampling variance drives differences in effects. Shaded regions represent the p-value of studies. The white region indicates studies where the p-value is between 0.1 and 1; dark gray where the p-value of studies is between 0.05 and 0.1 and the lighter gray regions where the p-value of studies is significant.

We can see from Fig. 2 above the typical funnel shape. You will notice that most effects lie in the positive correlation space – in other words there is a positive correlation between BMR and fitness. However, we also find some studies that show the opposite pattern. We expect that based on sampling theory alone, and indeed many of these effects fall close to the dotted sampling error intervals. Studies in the light grey regions are studies where the p-value was significant.

We might expect under a file-drawer situation (i.e., where researchers stash away poorer quality studies showing opposite effects in their desk drawers) that studies with low power (i.e., low precision, wide standard errors, and small sample sizes) and non-significant correlations will go unpublished. This should be particularly true for studies that show the opposite to what we might predict by theory – specifically, negative correlations from studies with small sample sizes / low precision that are not significant. This is one factor that can drive what we call funnel asymmetry, showing a bunch of missing effect sizes in the bottom left corner of the funnel.

# 8. Time-lag plot assessing how effect sizes may or may not have changed through time.

Time-lag bias is a very common form of publication bias that results from a change in the average effect size with the accumulation of new studies. Often, under-powered studies that find surprising results are published first, and these initial studies usually stimulate a swath of new experiments seeking to test whether such a pattern exists in a new study system. To some extent, this is a good thing, and is expected. We need studies to replicate the finding so we are not fooled by the result.

We know though that small studies are susceptible to huge sampling error. This can result in over-inflated effect sizes. As more studies accumulate, the average effect size usually converges on the ‘true’ mean.

Time-lag bias is usually depicted in two ways. First, using what is called a ‘cumulative meta-analysis’. This is where we conduct a meta-analysis on a subset of data adding to this data as we move through time. We usually visualise this using what is called a ‘cumulative forest plot’.

Second, we can also test whether the mean effect size changes with the year of publication using visual and meta-regression approaches.

Given the two methods are similar, we’ll focus on the second option to keep things simple.

Let’s first focus on some visuals in this task. Let’s visualize whether we see any relationship between average effect size and the year of publication? Use ggplot to do this. Scale the size of the points based on their sampling error. Remember to clearly label the axes and the legend.

```{r}
ggplot(lnRR, aes(y = lnRR, x = Year..print., size = 1/sqrt(V_lnRR))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Fisher's Log Response Ratio Correlation Coefficient (lnRR)", size = "Precision (1/SE)") +
    theme_classic() 
```

# 9. Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias

```{r}
library(metafor)
# Including sampling variance as moderator
meta_reg <- rma.mv(lnRR ~ Year..print., V = V_lnRR, random = list(~1 | Study, ~1 | residual),
    test = "t", dfs = "contain", data = lnRR)
summary(meta_reg)
```

# 10. Formal meta-regression model that includes inverse sampling variance (i.e., 1/vlnRR) to test for file-drawer biases 

```{r}
meta_reg_test <- rma.mv(lnRR ~ (1 / V_lnRR), V = V_lnRR, random = list(~1 | Study, ~1 | residual), test = "t", dfs = "contain", data = lnRR)
summary(meta_reg_test)
```

```{r}
# How much variation does time when results were published explain in Zr?
r2_time <- orchaRd::r2_ml(meta_reg_test)  
r2_time
```

# 11. A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

Meta-analysis bias, and the Cochrane Taxonomy, which emphasizes the integrity of included studies primarily through reporting bias, and classifies reporting bias as: publication bias, time lag bias, multiple/repeat publication bias, publication location bias, citation bias, language bias, and outcome reporting bias.

Among these biases, the most studied is publication bias, which is also a very important factor affecting the quality of Meta-analysis. publication bias refers to the greater likelihood that study results with statistically significant research significance will be reported and published than those with no significance and invalid results. The reliance on the direction and strength of the findings when researchers, reviewers, or editors select papers for publication creates a bias that makes the process of publication not a random event and therefore suppresses the publication of certain studies. Funnel plots are the most common method of identifying publication bias during Meta-analysis, and they respond to estimates of the intervention effects of individual studies for a given sample size or precision of the study.

When performing Meta-analysis, the possibility of bias must be tested for the presence of bias. However, publication bias is difficult to avoid, and for its identification, the most common method is the funnel plot method, which is a funnel plot expression that can be more intuitive to visually detect the presence or absence of bias. Theoretically, the point estimate of each independent study effect that was included in the Meta-analysis, the set in the plane coordinate system should be an inverted funnel shape, hence the name funnel plot. Small sample size and low study precision are distributed at the bottom of the funnel plot and scattered around; large sample size and high study precision are distributed at the top of the funnel plot and concentrated toward the middle. As can be seen from the funnel plots drawn in our study, we present asymmetric funnel plots and the asymmetry is very pronounced, indicating the presence of bias, which may overestimate the treatment effect.

However, publication bias is not the only reason for the asymmetry of the funnel plot; we have very large heterogeneity (I^2), which may be responsible for the asymmetry of the funnel plot. But looking at the white area in the funnel plot, there is the insignificant area where there are many dots, indicating that many articles are not published, suggesting that there may be publication bias.

Visually examining the meta-regression picture we have drawn, we find that there does seem to be a significant positive correlation between the difference in means and year for the experimental and control groups. Again the earlier studies have lower sampling variance (i.e. higher precision) and these earlier studies appear to have lower (accurate) effects compared to studies conducted in later years.

# 12. Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

























